import cv2
import json
import os
import sys
import pyttsx3  # Import pyttsx3

# Initialize the text-to-speech engine
engine = pyttsx3.init()

# Optionally set properties like rate and volume
engine.setProperty('rate', 150)    # Speed of speech
engine.setProperty('volume', 1.0)  # Volume level (0.0 to 1.0)

# --- –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º ---
MODEL_PATH = 'trained_model.yml'
LABELS_PATH = 'labels.json'

# --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ---
CONFIDENCE_THRESHOLD = 80
FRAME_WIDTH = 640
FRAME_HEIGHT = 480

# --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ ---
def load_resources():
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤...")

    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –º–æ–¥–µ–ª—å: {MODEL_PATH}")
        sys.exit(1)
    recognizer = cv2.face.LBPHFaceRecognizer_create()
    recognizer.read(MODEL_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {MODEL_PATH}")

    if not os.path.exists(LABELS_PATH):
        print(f"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã –º–µ—Ç–∫–∏: {LABELS_PATH} ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å")
        label_names = {}
    else:
        with open(LABELS_PATH, 'r', encoding='utf-8') as f:
            try:
                label_names = json.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–µ—Ç–æ–∫: {len(label_names)}")
            except json.JSONDecodeError as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ JSON: {e}")
                sys.exit(1)

    return face_cascade, recognizer, label_names

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def main():
    face_cascade, recognizer, label_names = load_resources()

    cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)

    if not cap.isOpened():
        print("‚ùå –ö–∞–º–µ—Ä–∞ –Ω–µ –æ—Ç–∫—Ä—ã—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π CAP_V4L –≤–º–µ—Å—Ç–æ CAP_V4L2.")
        cap.open(0, cv2.CAP_V4L)
        if not cap.isOpened():
            sys.exit(1)

    print("üé• –ö–∞–º–µ—Ä–∞ –∑–∞–ø—É—â–µ–Ω–∞. –ù–∞–∂–º–∏—Ç–µ 'q', —á—Ç–æ–±—ã –≤—ã–π—Ç–∏.")

    cv2.namedWindow('–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü', cv2.WINDOW_AUTOSIZE)

    # Keep track of who has been greeted
    greeted = set()

    while True:
        ret, frame = cap.read()
        if not ret:
            print("‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–∞")
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        faces = face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.2,
            minNeighbors=5,
            minSize=(60, 60),
            flags=cv2.CASCADE_SCALE_IMAGE
        )

        for (x, y, w, h) in faces:
            face_roi = gray[y:y+h, x:x+w]
            label_id, confidence = recognizer.predict(face_roi)

            if confidence < CONFIDENCE_THRESHOLD:
                name = label_names.get(str(label_id), "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                text = f"{name} ({confidence:.1f})"
                color = (0, 255, 0)

                # Only greet if this person hasn't been greeted yet
                if name not in greeted:
                    greeting = f"Hello, {name}!"
                    print(greeting)
                    engine.say(greeting)
                    engine.runAndWait()
                    greeted.add(name)  # Mark as greeted
            else:
                text = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
                color = (0, 0, 255)

            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(frame, text, (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)

        cv2.imshow('–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü', frame)

        key = cv2.waitKey(1) & 0xFF
        if key == ord('q') or key == 27:
            break

    cap.release()
    cv2.destroyAllWindows()
    print("üëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

if __name__ == "__main__":
    main()